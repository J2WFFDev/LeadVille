            # Enhanced per-sensor calibration analysis with improved sensor detection
            print()  # New line after progress
            print(f"\n🎯 Calibration collection completed for all sensors!")
            print(f"\n📊 Detailed Per-Sensor Calibration Analysis:")
            print(f"📈 Total samples collected: {len(self.calibration_samples)}")
            
            # Analyze calibration data by clustering samples (sensors typically have different baseline characteristics)
            vx_values = [s['vx_raw'] for s in self.calibration_samples]
            vy_values = [s['vy_raw'] for s in self.calibration_samples]
            vz_values = [s['vz_raw'] for s in self.calibration_samples]
            
            # Enhanced sensor detection using value clustering instead of time clustering
            import statistics
            from collections import defaultdict
            
            # Try to identify sensors by X-axis value clustering (each sensor has different baseline)
            # Sort samples by vx_raw value to find natural clusters
            vx_sorted = sorted(self.calibration_samples, key=lambda x: x['vx_raw'])
            
            # Find the gap in X values to split into two sensor groups
            vx_values_sorted = [s['vx_raw'] for s in vx_sorted]
            
            # Calculate differences between consecutive values
            if len(vx_values_sorted) > 1:
                diffs = [vx_values_sorted[i+1] - vx_values_sorted[i] for i in range(len(vx_values_sorted)-1)]
                max_gap_idx = diffs.index(max(diffs))
                split_value = (vx_values_sorted[max_gap_idx] + vx_values_sorted[max_gap_idx + 1]) / 2
                
                # Split samples into two groups based on X value
                sensor_groups = defaultdict(list)
                for sample in self.calibration_samples:
                    if sample['vx_raw'] <= split_value:
                        sensor_groups[0].append(sample)
                    else:
                        sensor_groups[1].append(sample)
                        
                print(f"📊 Sensor detection: Split at X={split_value:.0f}")
            else:
                # Fallback: all samples in one group
                sensor_groups = {0: self.calibration_samples}
                print(f"📊 Sensor detection: Single sensor pattern detected")
            
            # Also try timing-based detection as secondary method
            sorted_samples = sorted(self.calibration_samples, key=lambda x: x['timestamp'])
            time_groups = defaultdict(list)
            
            # Simple time-based grouping with smaller intervals
            current_group = 0
            time_groups[current_group].append(sorted_samples[0])
            
            for i in range(1, len(sorted_samples)):
                delta = sorted_samples[i]['timestamp'] - sorted_samples[i-1]['timestamp']
                
                # Switch groups based on time patterns (sensors may alternate)
                if delta > 0.05:  # 50ms gap suggests sensor switch
                    current_group = 1 - current_group
                    
                time_groups[current_group].append(sorted_samples[i])
            
            print(f"📊 Time-based groups: {len(time_groups[0])} + {len(time_groups.get(1, []))}")
            print(f"📊 Value-based groups: {len(sensor_groups[0])} + {len(sensor_groups.get(1, []))}")
            
            # Use the grouping method that gives more balanced results
            if len(sensor_groups) == 2 and min(len(sensor_groups[0]), len(sensor_groups[1])) > 5:
                selected_groups = sensor_groups
                detection_method = "Value clustering"
            elif len(time_groups) == 2 and min(len(time_groups[0]), len(time_groups.get(1, []))) > 5:
                selected_groups = time_groups
                detection_method = "Time pattern"
            else:
                # Fallback: create artificial split for analysis
                mid_point = len(self.calibration_samples) // 2
                selected_groups = {
                    0: self.calibration_samples[:mid_point],
                    1: self.calibration_samples[mid_point:]
                }
                detection_method = "Sequential split"
            
            print(f"📊 Using {detection_method} for sensor separation")
            
            # Analyze each sensor group
            sensor_stats = {}
            for group_id, samples in selected_groups.items():
                if len(samples) < 5:  # Skip groups with too few samples
                    continue
                    
                vx_group = [s['vx_raw'] for s in samples]
                vy_group = [s['vy_raw'] for s in samples]
                vz_group = [s['vz_raw'] for s in samples]
                
                # Calculate statistics for this sensor group
                baseline_x = int(sum(vx_group) / len(vx_group))
                baseline_y = int(sum(vy_group) / len(vy_group))
                baseline_z = int(sum(vz_group) / len(vz_group))
                
                noise_x = statistics.stdev(vx_group) if len(set(vx_group)) > 1 else 0
                noise_y = statistics.stdev(vy_group) if len(set(vy_group)) > 1 else 0
                noise_z = statistics.stdev(vz_group) if len(set(vz_group)) > 1 else 0
                
                # Store sensor statistics
                sensor_stats[group_id] = {
                    'samples': len(samples),
                    'baseline_x': baseline_x,
                    'baseline_y': baseline_y,
                    'baseline_z': baseline_z,
                    'noise_x': noise_x,
                    'noise_y': noise_y,
                    'noise_z': noise_z,
                    'vx_range': (min(vx_group), max(vx_group)),
                    'vy_range': (min(vy_group), max(vy_group)),
                    'vz_range': (min(vz_group), max(vz_group))
                }
            
            # Display per-sensor results
            sensor_names = ['BAE5', '5550']  # Known sensor identifiers
            for i, (group_id, stats) in enumerate(sensor_stats.items()):
                sensor_name = sensor_names[i] if i < len(sensor_names) else f"Sensor_{group_id+1}"
                
                print(f"\n📊 {sensor_name} Sensor Analysis:")
                print(f"   📈 Samples collected: {stats['samples']}")
                print(f"   📍 Baseline: X={stats['baseline_x']:+05d}, Y={stats['baseline_y']:+05d}, Z={stats['baseline_z']:+05d}")
                print(f"   📏 Noise (±1σ): X=±{stats['noise_x']:.1f}, Y=±{stats['noise_y']:.1f}, Z=±{stats['noise_z']:.1f}")
                print(f"   📊 Raw ranges: X={stats['vx_range']}, Y={stats['vy_range']}, Z={stats['vz_range']}")
                
                # Calculate adjustment needed from zero
                adj_x = abs(stats['baseline_x'])
                adj_y = abs(stats['baseline_y']) 
                adj_z = abs(stats['baseline_z'])
                print(f"   🔧 Zero adjustment: X={adj_x}, Y={adj_y}, Z={adj_z} counts")
                
                # Confidence intervals (±2σ = ~95% confidence)
                conf_x = 2 * stats['noise_x']
                conf_y = 2 * stats['noise_y']
                conf_z = 2 * stats['noise_z']
                print(f"   📈 95% confidence (±2σ): X=±{conf_x:.1f}, Y=±{conf_y:.1f}, Z=±{conf_z:.1f}")

            # Calculate combined baseline for system compatibility
            self.baseline_x = int(sum(vx_values) / len(vx_values))
            self.baseline_y = int(sum(vy_values) / len(vy_values))
            self.baseline_z = int(sum(vz_values) / len(vz_values))

            # Combined noise characteristics
            noise_x = statistics.stdev(vx_values) if len(set(vx_values)) > 1 else 0
            noise_y = statistics.stdev(vy_values) if len(set(vy_values)) > 1 else 0
            noise_z = statistics.stdev(vz_values) if len(set(vz_values)) > 1 else 0
            
            print(f"\n📊 Combined System Baseline:")
            print(f"   📍 System baseline: X={self.baseline_x:+05d}, Y={self.baseline_y:+05d}, Z={self.baseline_z:+05d}")
            print(f"   📏 Combined noise: X=±{noise_x:.1f}, Y=±{noise_y:.1f}, Z=±{noise_z:.1f}")
            print(f"   🎯 Impact threshold: {IMPACT_THRESHOLD} counts from baseline")